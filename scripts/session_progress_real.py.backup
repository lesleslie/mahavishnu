#!/usr/bin/env python3
"""
Claude Code Session Progress StatusLine - Transcript-Based Token Tracking
Shows accurate token usage from conversation transcript JSONL files

Architecture:
- Extract: Stream JSONL records from transcript file
- Transform: Aggregate usage data from assistant messages
- Load: Format progress data for statusLine display

Author: python-pro specialist (based on data-engineer analysis)
Date: 2025-10-03
"""
import sys
import json
import os
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Iterator, Optional

# Token budget (200k tokens per 5-hour session window)
TOKEN_BUDGET = 200_000

# Session window duration (5 hours from start)
SESSION_DURATION_HOURS = 5


def format_time(dt: datetime) -> str:
    """
    Format time as 12-hour with AM/PM in local timezone
    Rounds up to next hour (ceiling) to match Claude Code UI
    """
    # Convert UTC datetime to local timezone
    local_dt = dt.astimezone()

    # Round up to next hour if not already on the hour
    if local_dt.minute > 0 or local_dt.second > 0 or local_dt.microsecond > 0:
        rounded_dt = local_dt.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
    else:
        rounded_dt = local_dt

    return rounded_dt.strftime("%I%p").lstrip('0').lower()


def create_progress_bar(percentage: float, width: int = 15) -> str:
    """Create smooth progress bar using alive-progress smooth theme characters"""
    percentage = min(percentage, 100)
    filled_width = (width * percentage) / 100

    full_blocks = int(filled_width)
    remainder = filled_width - full_blocks

    # Smooth characters from alive-progress smooth theme
    smooth_chars = " ‚ñè‚ñé‚ñç‚ñå‚ñã‚ñä‚ñâ‚ñà"

    # Build the bar
    bar = "‚ñà" * full_blocks

    # Add partial block for smooth transition
    if full_blocks < width and remainder > 0:
        partial_index = int(remainder * (len(smooth_chars) - 1))
        bar += smooth_chars[partial_index]
        empty_width = width - full_blocks - 1
    else:
        empty_width = width - full_blocks

    # Add empty space
    bar += " " * empty_width

    return bar


def validate_transcript_file(path: Path) -> bool:
    """Validate that a transcript file exists, is readable, and non-empty"""
    try:
        return path.exists() and path.is_file() and path.stat().st_size > 0
    except (OSError, PermissionError):
        return False


def discover_transcript_path(stdin_data: dict) -> Optional[str]:
    """
    Multi-method transcript path discovery with graceful fallbacks

    Priority:
    1. stdin JSON data (transcriptPath or sessionId+projectPath)
    2. Environment variables
    3. Filesystem search (most recent .jsonl with depth limit)
    """
    # Method 1: Check stdin for direct transcript path
    # Support both camelCase (legacy) and snake_case (current)
    transcript_key = 'transcript_path' if 'transcript_path' in stdin_data else 'transcriptPath'
    if transcript_key in stdin_data:
        path = Path(stdin_data[transcript_key])
        if validate_transcript_file(path):
            return str(path)

    # Method 2: Build path from stdin components
    # Support both camelCase (legacy) and snake_case (current)
    session_id = stdin_data.get('session_id') or stdin_data.get('sessionId')
    project_path = stdin_data.get('project_path') or stdin_data.get('projectPath')

    if session_id and project_path:
        transcript_path = (
            Path.home() / '.claude' / 'projects' /
            project_path / f'{session_id}.jsonl'
        )
        if validate_transcript_file(transcript_path):
            return str(transcript_path)

    # Method 3: Check environment variables
    session_id = os.environ.get('CLAUDE_SESSION_ID')
    project_path = os.environ.get('CLAUDE_PROJECT_PATH')

    if session_id and project_path:
        transcript_path = (
            Path.home() / '.claude' / 'projects' /
            project_path / f'{session_id}.jsonl'
        )
        if validate_transcript_file(transcript_path):
            return str(transcript_path)

    # Method 4: Filesystem search with depth limit (performance optimization)
    # Only search immediate project subdirectories, not deep recursion
    projects_dir = Path.home() / '.claude' / 'projects'
    if projects_dir.exists():
        try:
            # Limit search to depth 2: projects/*/session.jsonl
            # Use generator to avoid materializing entire list
            most_recent = None
            most_recent_time = 0

            for project_subdir in projects_dir.iterdir():
                if not project_subdir.is_dir():
                    continue

                for jsonl_file in project_subdir.glob('*.jsonl'):
                    if validate_transcript_file(jsonl_file):
                        mtime = jsonl_file.stat().st_mtime
                        if mtime > most_recent_time:
                            most_recent = jsonl_file
                            most_recent_time = mtime

            if most_recent:
                return str(most_recent)
        except (OSError, PermissionError):
            pass

    return None


def extract_transcript_records(jsonl_path: str) -> Iterator[dict]:
    """
    Stream JSONL records from transcript file

    Handles malformed lines gracefully (file may be actively written)
    Performance: O(1) memory via generator, processes files of any size
    """
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                # Skip empty lines (common in actively-written files)
                line = line.strip()
                if not line:
                    continue

                try:
                    yield json.loads(line)
                except json.JSONDecodeError:
                    # Skip malformed lines (partial writes during concurrent access)
                    continue
    except (FileNotFoundError, PermissionError, OSError):
        # Graceful fallback - return empty iterator
        return


def aggregate_token_usage(records: Iterator[dict]) -> dict:
    """
    Aggregate token usage from assistant messages

    Formula (validated by data-engineer):
    billable_tokens = input_tokens + cache_creation_input_tokens + output_tokens

    Note: cache_read_input_tokens are NOT included (90% discount)

    Handles streaming duplicates: Only counts the last record per assistant message
    (streaming creates multiple records with identical usage until final output)

    Performance: O(n) where n = number of records, O(1) memory (streaming)

    Returns:
        {
            'session_start': datetime | None,
            'session_end': datetime | None,
            'input_tokens': int,
            'cache_creation_tokens': int,
            'cache_read_tokens': int,
            'output_tokens': int,
            'billable_tokens': int,
            'message_count': int
        }
    """
    totals = {
        'input': 0,
        'cache_creation': 0,
        'cache_read': 0,
        'output': 0,
        'messages': 0
    }

    session_start = None
    session_end = None

    # Track last usage to detect and skip duplicates from streaming
    last_usage_str = None

    for record in records:
        # Track session timing from timestamps
        if 'timestamp' in record:
            try:
                ts_str = record['timestamp']
                # Handle both 'Z' suffix and explicit timezone offsets
                if isinstance(ts_str, str):
                    # Replace 'Z' with UTC offset for ISO 8601 compliance
                    ts_str = ts_str.replace('Z', '+00:00')
                    ts = datetime.fromisoformat(ts_str)

                    if session_start is None or ts < session_start:
                        session_start = ts
                    if session_end is None or ts > session_end:
                        session_end = ts
            except (ValueError, AttributeError, TypeError):
                # Gracefully skip malformed timestamps
                pass

        # Aggregate tokens from assistant messages only
        # Use defensive checks to handle malformed records
        if (record.get('type') == 'assistant' and
            isinstance(record.get('message'), dict) and
            isinstance(record['message'].get('usage'), dict)):

            usage = record['message']['usage']

            # Detect streaming duplicates: serialize usage to compare with previous
            # Streaming creates multiple records with identical usage until final output
            usage_str = json.dumps(usage, sort_keys=True)

            # Skip if this is a duplicate of the last usage record
            if usage_str == last_usage_str:
                continue

            last_usage_str = usage_str

            # Ensure all token counts are integers (defensive)
            totals['input'] += int(usage.get('input_tokens', 0) or 0)
            totals['cache_creation'] += int(usage.get('cache_creation_input_tokens', 0) or 0)
            totals['cache_read'] += int(usage.get('cache_read_input_tokens', 0) or 0)
            totals['output'] += int(usage.get('output_tokens', 0) or 0)
            totals['messages'] += 1

    # Calculate billable tokens (input + cache_creation + output)
    # Cache reads excluded - they're discounted 90%
    billable = (
        totals['input'] +
        totals['cache_creation'] +
        totals['output']
    )

    return {
        'session_start': session_start,
        'session_end': session_end,
        'input_tokens': totals['input'],
        'cache_creation_tokens': totals['cache_creation'],
        'cache_read_tokens': totals['cache_read'],
        'output_tokens': totals['output'],
        'billable_tokens': billable,
        'message_count': totals['messages']
    }


def load_session_progress(jsonl_path: str) -> dict:
    """
    Complete ETL pipeline: extract ‚Üí transform ‚Üí load

    Returns dict ready for statusLine display:
        {
            'tokens_used': int,
            'tokens_budget': int,
            'percentage': float,
            'session_start': datetime | None,
            'reset_time': datetime | None,
            'time_remaining': timedelta,
            'exceeds_limit': bool,
            'message_count': int
        }
    """
    # Extract and transform
    records = extract_transcript_records(jsonl_path)
    stats = aggregate_token_usage(records)

    # Calculate progress
    tokens_used = stats['billable_tokens']
    percentage = min((tokens_used / TOKEN_BUDGET) * 100, 100)

    # Calculate session reset time (5 hours from start)
    reset_time = None
    time_remaining = timedelta(0)

    if stats['session_start']:
        reset_time = stats['session_start'] + timedelta(hours=SESSION_DURATION_HOURS)
        now = datetime.now(timezone.utc)
        time_remaining = reset_time - now
        time_remaining = max(time_remaining, timedelta(0))

    return {
        'tokens_used': tokens_used,
        'tokens_budget': TOKEN_BUDGET,
        'percentage': percentage,
        'session_start': stats['session_start'],
        'reset_time': reset_time,
        'time_remaining': time_remaining,
        'exceeds_limit': tokens_used >= TOKEN_BUDGET,
        'message_count': stats['message_count']
    }


def estimate_token_usage_from_cost(cost_usd: float) -> int:
    """
    Fallback: Estimate token count from cost

    NOTE: This is less accurate than transcript parsing but serves as fallback
    when transcript file is unavailable.

    Calibrated against /status: $23.77/million effective rate
    """
    if cost_usd == 0:
        return 0

    avg_cost_per_million = 23.77
    estimated_tokens = int((cost_usd / avg_cost_per_million) * 1_000_000)
    return estimated_tokens


def format_tokens_display(tokens: int, budget: int) -> str:
    """Format token count for compact display"""
    if tokens >= 1000:
        return f"{tokens/1000:.1f}k/{budget/1000:.0f}k"
    else:
        return f"{tokens}/{budget}"


def main():
    try:
        # Read JSON input from stdin
        input_data = sys.stdin.read()
        data = json.loads(input_data)

        # Use cost-based estimation (matches Claude Code official UI)
        cost_usd = data.get("cost", {}).get("total_cost_usd", 0)
        exceeds_tokens = data.get("exceeds_200k_tokens", False)

        # Get session start for reset time calculation
        transcript_path = discover_transcript_path(data)
        reset_time = None

        if transcript_path:
            try:
                progress = load_session_progress(transcript_path)
                reset_time = progress.get('reset_time')
            except:
                pass  # Fallback to no reset time

        # Convert cost to effective tokens
        estimated_tokens = estimate_token_usage_from_cost(cost_usd)
        percentage = min((estimated_tokens / TOKEN_BUDGET) * 100, 100)

        # Status indicator
        if exceeds_tokens or percentage >= 100:
            status = "üî¥ LIMIT"
        elif percentage >= 90:
            status = "üî¥ HIGH"
        elif percentage >= 75:
            status = "üü° MED"
        else:
            status = "üü¢ OK"

        # Create progress bar
        bar = create_progress_bar(percentage)

        # Token display
        tokens_display = format_tokens_display(estimated_tokens, TOKEN_BUDGET)

        # Time display
        if reset_time:
            reset_display = format_time(reset_time)
        else:
            reset_display = "unknown"

        # ANSI codes
        bold_white = "\033[1m\033[97m"
        reset_ansi = "\033[0m"

        # Output: STATUS | BAR PERCENT% | TOKENS | Resets TIME
        output = (
            f"{bold_white}{status} ‚îÇ {bar} {percentage:.0f}% ‚îÇ "
            f"{tokens_display} ‚îÇ Resets {reset_display}{reset_ansi}"
        )
        print(output)

    except Exception as e:
        # Ultimate fallback
        print(f"‚ö†Ô∏è  Status error: {str(e)}", file=sys.stderr)
        print("Session active")


if __name__ == "__main__":
    main()
